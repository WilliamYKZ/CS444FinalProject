The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:02<00:00, 31.09s/it]
Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at /home/exouser/Desktop/DeepSeek-R1-Distill-Qwen-7B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Device set to use cuda:0
0it [00:00, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
0it [00:00, ?it/s]
Traceback (most recent call last):
  File "/home/exouser/Desktop/20_Advantage/Experiment.py", line 241, in <module>
    response_tensors = ppo_trainer.generate(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 495, in generate
    response = self._generate_batched(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 582, in _generate_batched
    generations = unwrapped_model.generate(**padded_inputs, **generation_kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/models/modeling_value_head.py", line 207, in generate
    return self.pretrained_model.generate(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/peft/peft_model.py", line 1838, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/utils.py", line 2148, in generate
    prepared_logits_processor = self._get_logits_processor(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/utils.py", line 1061, in _get_logits_processor
    TopKLogitsWarper(top_k=generation_config.top_k, min_tokens_to_keep=min_tokens_to_keep)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/logits_process.py", line 523, in __init__
    raise ValueError(f"`top_k` has to be a strictly positive integer, but is {top_k}")
ValueError: `top_k` has to be a strictly positive integer, but is 1.0
Traceback (most recent call last):
  File "/home/exouser/Desktop/20_Advantage/Experiment.py", line 241, in <module>
    response_tensors = ppo_trainer.generate(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 495, in generate
    response = self._generate_batched(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 582, in _generate_batched
    generations = unwrapped_model.generate(**padded_inputs, **generation_kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/models/modeling_value_head.py", line 207, in generate
    return self.pretrained_model.generate(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/peft/peft_model.py", line 1838, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/utils.py", line 2148, in generate
    prepared_logits_processor = self._get_logits_processor(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/utils.py", line 1061, in _get_logits_processor
    TopKLogitsWarper(top_k=generation_config.top_k, min_tokens_to_keep=min_tokens_to_keep)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/logits_process.py", line 523, in __init__
    raise ValueError(f"`top_k` has to be a strictly positive integer, but is {top_k}")
ValueError: `top_k` has to be a strictly positive integer, but is 1.0
