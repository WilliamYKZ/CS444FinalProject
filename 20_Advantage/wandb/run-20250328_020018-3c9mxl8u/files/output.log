The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:17<00:00,  8.53s/it]
Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at /home/exouser/Desktop/DeepSeek-R1-Distill-Qwen-7B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Device set to use cuda:0
Epoch 0:   0%|                                                                                                                                                         | 0/8 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
Epoch 0:   0%|                                                                                                                                                         | 0/8 [01:57<?, ?it/s]
Traceback (most recent call last):
  File "/home/exouser/Desktop/20_Advantage/Experiment.py", line 304, in <module>
    tokenized = tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(model.device)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1931, in __getattr__
    raise AttributeError(
AttributeError: 'AutoModelForCausalLMWithValueHead' object has no attribute 'device'
Traceback (most recent call last):
  File "/home/exouser/Desktop/20_Advantage/Experiment.py", line 304, in <module>
    tokenized = tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(model.device)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1931, in __getattr__
    raise AttributeError(
AttributeError: 'AutoModelForCausalLMWithValueHead' object has no attribute 'device'
