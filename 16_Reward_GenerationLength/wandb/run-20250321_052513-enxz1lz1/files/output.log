/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.
  warnings.warn(
Using total PPO steps: 90
Resolving data files: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 25.18it/s]
Loading dataset shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 42/42 [00:00<00:00, 261.88it/s]
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:07<00:00,  3.73s/it]
WARNING:root:A <class 'peft.peft_model.PeftModelForCausalLM'> model is loaded from '/home/exouser/Desktop/0_ModelCheckpoint/Full_RL_Model/CheckPoint_500', and no v_head weight is found. This IS expected if you are not resuming PPO training.
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.
  warnings.warn(
The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:15<00:00,  7.73s/it]
Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at /home/exouser/Desktop/DeepSeek-R1-Distill-Qwen-7B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Device set to use cuda:0
0it [00:00, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
1it [00:51, 51.02s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.19 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.
  warnings.warn(
3it [03:42, 74.25s/it]
Traceback (most recent call last):
  File "/home/exouser/Desktop/16_Reward_GenerationLength/rl_training.py", line 193, in <module>
    response_tensors = ppo_trainer.generate(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 495, in generate
    response = self._generate_batched(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 582, in _generate_batched
    generations = unwrapped_model.generate(**padded_inputs, **generation_kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/models/modeling_value_head.py", line 207, in generate
    return self.pretrained_model.generate(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/peft/peft_model.py", line 1838, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/utils.py", line 2255, in generate
    result = self._sample(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/utils.py", line 3257, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 816, in forward
    outputs = self.model(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 562, in forward
    layer_outputs = self._gradient_checkpointing_func(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 264, in forward
    outputs = run_function(*args)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 259, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 162, in forward
    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/bitsandbytes/nn/modules.py", line 990, in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py", line 509, in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py", line 376, in forward
    output = F.int8_mm_dequant(out32, SCA, state.SCB, bias=None).to(A.dtype).add_(bias)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/exouser/Desktop/16_Reward_GenerationLength/rl_training.py", line 193, in <module>
    response_tensors = ppo_trainer.generate(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 495, in generate
    response = self._generate_batched(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 582, in _generate_batched
    generations = unwrapped_model.generate(**padded_inputs, **generation_kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/models/modeling_value_head.py", line 207, in generate
    return self.pretrained_model.generate(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/peft/peft_model.py", line 1838, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/utils.py", line 2255, in generate
    result = self._sample(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/utils.py", line 3257, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 816, in forward
    outputs = self.model(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 562, in forward
    layer_outputs = self._gradient_checkpointing_func(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 264, in forward
    outputs = run_function(*args)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 259, in forward
    hidden_states, self_attn_weights = self.self_attn(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 162, in forward
    key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/bitsandbytes/nn/modules.py", line 990, in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py", line 509, in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py", line 376, in forward
    output = F.int8_mm_dequant(out32, SCA, state.SCB, bias=None).to(A.dtype).add_(bias)
KeyboardInterrupt
