The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.
Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:59<00:00, 29.81s/it]
Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at /home/exouser/Desktop/DeepSeek-R1-Distill-Qwen-7B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Device set to use cuda:0
0it [00:00, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.
  warnings.warn(
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
1it [00:40, 40.10s/it]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
6it [05:21, 56.89s/it]/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.22 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.
  warnings.warn(
10it [08:55, 52.50s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.53 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.
  warnings.warn(
12it [10:56, 57.13s/it]/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py:1313: UserWarning: KL divergence is starting to become negative: -1.15 - this might be a precursor for failed training. sometimes this happens because the generation kwargs are not correctly set. Please make sure that the generation kwargs are set correctly, or review your training hyperparameters.
  warnings.warn(
13it [12:27, 57.50s/it]
Traceback (most recent call last):
  File "/home/exouser/Desktop/16_Reward_GenerationLength/rl_training.py", line 192, in <module>
    response_tensors = ppo_trainer.generate(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 495, in generate
    response = self._generate_batched(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 582, in _generate_batched
    generations = unwrapped_model.generate(**padded_inputs, **generation_kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/models/modeling_value_head.py", line 207, in generate
    return self.pretrained_model.generate(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/peft/peft_model.py", line 1838, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/utils.py", line 2255, in generate
    result = self._sample(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/utils.py", line 3257, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 816, in forward
    outputs = self.model(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 562, in forward
    layer_outputs = self._gradient_checkpointing_func(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/exouser/Desktop/16_Reward_GenerationLength/rl_training.py", line 192, in <module>
    response_tensors = ppo_trainer.generate(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 495, in generate
    response = self._generate_batched(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/trainer/ppo_trainer.py", line 582, in _generate_batched
    generations = unwrapped_model.generate(**padded_inputs, **generation_kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/trl/models/modeling_value_head.py", line 207, in generate
    return self.pretrained_model.generate(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/peft/peft_model.py", line 1838, in generate
    outputs = self.base_model.generate(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/utils.py", line 2255, in generate
    result = self._sample(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/generation/utils.py", line 3257, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 816, in forward
    outputs = self.model(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 562, in forward
    layer_outputs = self._gradient_checkpointing_func(
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
  File "/home/exouser/miniconda3/envs/PPO/lib/python3.9/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
KeyboardInterrupt
